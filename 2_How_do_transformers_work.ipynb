{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer learning**\n",
        "\n",
        "Transfer learning is a powerful technique that allows models to leverage knowledge acquired from one task to improve performance on another.\n",
        "\n",
        "Transformers Model like (GPT, BERT, BART, T5, etc.) have been trained as language models like they have been trained in a self supervised way in which the output is automatically computed from input without intervention, That means that humans are not needed to label the data!\n",
        "\n",
        "This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
        "\n",
        "# **Fine-tuning**\n",
        "Fine-tuning is like taking a well-trained expert and giving them some extra training to specialize in a specific task."
      ],
      "metadata": {
        "id": "n43Ao8dI2pTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sharing language model**\n",
        "\n",
        "Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!\n",
        "\n",
        "This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community."
      ],
      "metadata": {
        "id": "hpJgakhn4ALe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Epoch**\n",
        "\n",
        "An epoch in machine learning and deep learning is a complete pass through the entire training dataset. During training, the model's parameters are updated to improve its performance on the task at hand. Here’s a simple way to understand it:\n",
        "\n",
        "Dataset: Imagine you have a dataset of 1000 images.\n",
        "Batch Size: You choose to train the model on 100 images at a time (a batch).\n",
        "Epoch: One epoch means the model has seen and processed all 1000 images once.\n",
        "If you set the training to run for 10 epochs, it means the model will go through the entire dataset 10 times. Multiple epochs help the model learn and refine its parameters more effectively. However, too many epochs can lead to overfitting, where the model learns the training data too well and performs poorly on new, unseen data."
      ],
      "metadata": {
        "id": "yPr_gztD5zsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention layer**\n",
        "\n",
        "This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
        "\n",
        "To put this into context, consider the task of translating text from English to Urdu. Given the input “You like this course”, now for translation of \"You\" translator need the the the person is male or female but for course translation only course is required"
      ],
      "metadata": {
        "id": "xEv7qsbGBAQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Architecture Model**\n",
        "\n",
        "In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence)\n",
        "\n",
        "The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "**Architecture**: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
        "\n",
        "**Checkpoints**: These are the weights that will be loaded in a given architecture.\n",
        "\n",
        "**Model**: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
        "\n",
        "For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”"
      ],
      "metadata": {
        "id": "tqdZhdw-DsoT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lI5tesAr21Yr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}